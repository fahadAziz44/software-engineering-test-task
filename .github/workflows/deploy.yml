name: CD - Deploy to GKE

# ‚ö†Ô∏è DEPLOYMENT PAUSED ‚ö†Ô∏è
# This workflow manages production deployments.
# To resume deployments:
#   1. Add GKE_SA_KEY secret to GitHub repository settings
#   2. Update GKE_CLUSTER and GKE_REGION with your cluster details
#   3. Remove the `if: false` conditions from deploy-staging and deploy-production jobs
#
# The deployment code remains visible to demonstrate the CI/CD practices used when the service was live:
#   - Progressive deployment (staging ‚Üí production)
#   - Manual approval gates for production
#   - Zero-downtime rolling updates
#   - Automated smoke tests and health checks

on:
  push:
    branches:
      - master
  # No pull_request trigger - this only runs after merge to master

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository_owner }}/cruder
  GKE_CLUSTER: autopilot-cluster-1
  GKE_REGION: europe-central2

jobs:
  # Job 1: Build and push Docker image
  # Note: Tests/linting already passed in ci.yml workflow
  build:
    name: Build Docker Image
    runs-on: ubuntu-latest

    permissions:
      contents: read
      packages: write

    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      short-sha: ${{ steps.vars.outputs.short_sha }}
      owner: ${{ steps.repo.outputs.owner }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set repository owner (lowercase)
        id: repo
        run: |
          echo "owner=$(echo '${{ github.repository_owner }}' | tr '[:upper:]' '[:lower:]')" >> $GITHUB_OUTPUT

      - name: Set output variables
        id: vars
        run: echo "short_sha=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ steps.repo.outputs.owner }}/cruder
          tags: |
            type=sha,format=short
            type=raw,value=latest

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64 # build for amd64 architecture as GKE nodes run on amd64
          push: true
          tags: |
            ${{ env.REGISTRY }}/${{ steps.repo.outputs.owner }}/cruder:${{ steps.vars.outputs.short_sha }}
            ${{ env.REGISTRY }}/${{ steps.repo.outputs.owner }}/cruder:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Image digest
        run: |
          echo "‚úÖ Built and pushed image:"
          echo "${{ env.REGISTRY }}/${{ steps.repo.outputs.owner }}/cruder:${{ steps.vars.outputs.short_sha }}"

  # Job 2: Deploy to Staging (automatic)
  # ‚ö†Ô∏è PAUSED FOR COST OPTIMIZATION ‚ö†Ô∏è
  # This job handles staging deployment.
  # Remove `if: false` and ensure GKE_SA_KEY secret exists to enable.
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: build
    if: false  # Paused for cost optimization - remove this line to enable deployments
    environment:
      name: staging
      url: http://<STAGING_IP>  # Replace with your staging Load Balancer IP

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GKE_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Install gke-gcloud-auth-plugin
        run: |
          gcloud components install gke-gcloud-auth-plugin

      - name: Get GKE credentials
        run: |
          gcloud container clusters get-credentials ${{ env.GKE_CLUSTER }} \
            --region=${{ env.GKE_REGION }}

      - name: Update deployment image
        run: |
          kubectl set image deployment/cruder-app \
            cruder-app=${{ env.REGISTRY }}/${{ needs.build.outputs.owner }}/cruder:${{ needs.build.outputs.short-sha }} \
            -n staging

      - name: Wait for rollout to complete
        run: |
          kubectl rollout status deployment/cruder-app -n staging --timeout=300s

      - name: Verify deployment
        run: |
          echo "Checking deployment status..."

          # Get pod status (for visibility, but don't fail on transient states)
          kubectl get pods -n staging -l app=cruder

          # Verify all replicas are ready using deployment-level status (not pod-level)
          # Why: During rolling updates, kubectl wait with pod labels matches both old (terminating)
          # and new pods, causing "pod not found" errors. Deployment status only counts pods
          # from the current ReplicaSet, avoiding race conditions. See docs/incidents/2025-01-27-deployment-verification-fix.md
          READY_REPLICAS=$(kubectl get deployment cruder-app -n staging -o jsonpath='{.status.readyReplicas}')
          DESIRED_REPLICAS=$(kubectl get deployment cruder-app -n staging -o jsonpath='{.spec.replicas}')

          echo "Ready replicas: $READY_REPLICAS/$DESIRED_REPLICAS"

          if [ "$READY_REPLICAS" -eq "$DESIRED_REPLICAS" ]; then
            echo "‚úÖ All replicas are ready"
          else
            echo "‚ùå Not all replicas are ready"
            exit 1
          fi

      - name: Run smoke tests
        run: |
          echo "Running smoke tests against staging..."

          # Test health endpoint
          kubectl run test-pod --image=curlimages/curl:latest --rm -i --restart=Never -- \
            curl -f http://cruder-service.staging.svc.cluster.local/health || exit 1

          # Test ready endpoint
          kubectl run test-pod --image=curlimages/curl:latest --rm -i --restart=Never -- \
            curl -f http://cruder-service.staging.svc.cluster.local/ready || exit 1

          echo "‚úÖ Smoke tests passed"

      - name: Deployment summary
        run: |
          echo "::notice::‚úÖ Successfully deployed to staging"
          echo "::notice::Image: ${{ env.REGISTRY }}/${{ needs.build.outputs.owner }}/cruder:${{ needs.build.outputs.short-sha }}"
          echo "::notice::URL: http://<STAGING_IP>"  # Replace with your staging Load Balancer IP

  # Job 3: Deploy to Production (manual approval required)
  # ‚ö†Ô∏è PAUSED FOR COST OPTIMIZATION ‚ö†Ô∏è
  # This job handles production deployment with manual approval gates.
  # Remove `if: false` and ensure GKE_SA_KEY secret exists to enable.
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [build, deploy-staging]
    if: false  # Paused for cost optimization - remove this line to enable deployments
    environment:
      name: production
      url: http://<PRODUCTION_IP>  # Replace with your production Load Balancer IP

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GKE_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Install gke-gcloud-auth-plugin
        run: |
          gcloud components install gke-gcloud-auth-plugin

      - name: Get GKE credentials
        run: |
          gcloud container clusters get-credentials ${{ env.GKE_CLUSTER }} \
            --region=${{ env.GKE_REGION }}

      - name: Update deployment image
        run: |
          kubectl set image deployment/cruder-app \
            cruder-app=${{ env.REGISTRY }}/${{ needs.build.outputs.owner }}/cruder:${{ needs.build.outputs.short-sha }} \
            -n production

      - name: Wait for rollout to complete
        run: |
          kubectl rollout status deployment/cruder-app -n production --timeout=600s

      - name: Verify deployment
        run: |
          echo "Checking deployment status..."

          # Get pod status (for visibility, but don't fail on transient states)
          kubectl get pods -n production -l app=cruder

          # Verify all replicas are ready using deployment-level status (not pod-level)
          # Why: During rolling updates, kubectl wait with pod labels matches both old (terminating)
          # and new pods, causing "pod not found" errors. Deployment status only counts pods
          # from the current ReplicaSet, avoiding race conditions. See docs/incidents/2025-01-27-deployment-verification-fix.md
          READY_REPLICAS=$(kubectl get deployment cruder-app -n production -o jsonpath='{.status.readyReplicas}')
          DESIRED_REPLICAS=$(kubectl get deployment cruder-app -n production -o jsonpath='{.spec.replicas}')

          echo "Ready replicas: $READY_REPLICAS/$DESIRED_REPLICAS"

          if [ "$READY_REPLICAS" -eq "$DESIRED_REPLICAS" ]; then
            echo "‚úÖ All replicas are ready"
          else
            echo "‚ùå Not all replicas are ready"
            exit 1
          fi

      - name: Run smoke tests
        run: |
          echo "Running smoke tests against production..."

          # Test health endpoint
          kubectl run test-pod --image=curlimages/curl:latest --rm -i --restart=Never -- \
            curl -f http://cruder-service.production.svc.cluster.local/health || exit 1

          # Test ready endpoint
          kubectl run test-pod --image=curlimages/curl:latest --rm -i --restart=Never -- \
            curl -f http://cruder-service.production.svc.cluster.local/ready || exit 1

          echo "‚úÖ Smoke tests passed"

      - name: Deployment summary
        run: |
          echo "::notice::üöÄ Successfully deployed to PRODUCTION"
          echo "::notice::Image: ${{ env.REGISTRY }}/${{ needs.build.outputs.owner }}/cruder:${{ needs.build.outputs.short-sha }}"
          echo "::notice::URL: http://<PRODUCTION_IP>"  # Replace with your production Load Balancer IP

      - name: Notify on failure
        if: failure()
        run: |
          echo "::error::‚ùå Production deployment failed!"
          echo "::error::Consider rolling back with: kubectl rollout undo deployment/cruder-app -n production"
 
