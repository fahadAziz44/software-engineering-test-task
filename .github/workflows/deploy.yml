name: CD - Deploy to GKE

# ğŸ¯ PORTFOLIO SHOWCASE MODE
# This workflow demonstrates production-ready CI/CD practices:
#
# âœ… RUNS SUCCESSFULLY:
#   - Docker image build and push to GHCR
#   - Staging deployment workflow (showcase mode)
#   - Production deployment workflow (showcase mode with manual approval)
#
# ğŸ”§ TO ENABLE ACTUAL GKE DEPLOYMENTS:
#   1. Add GKE_SA_KEY secret to GitHub repository settings
#   2. Update GKE_CLUSTER and GKE_REGION with your cluster details
#   3. Uncomment the actual deployment code and replace showcase jobs
#
# ğŸ“š DEMONSTRATES:
#   - Progressive deployment strategy (staging â†’ production)
#   - Manual approval gates for production (GitHub Environments)
#   - Zero-downtime rolling updates with health probes
#   - Automated smoke tests and verification
#   - Immutable deployments with SHA-tagged Docker images

on:
  push:
    branches:
      - master
  # No pull_request trigger - this only runs after merge to master

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository_owner }}/cruder
  GKE_CLUSTER: autopilot-cluster-1
  GKE_REGION: europe-central2

jobs:
  # Job 1: Build and push Docker image
  # Note: Tests/linting already passed in ci.yml workflow
  build:
    name: Build Docker Image
    runs-on: ubuntu-latest

    permissions:
      contents: read
      packages: write

    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      short-sha: ${{ steps.vars.outputs.short_sha }}
      owner: ${{ steps.repo.outputs.owner }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set repository owner (lowercase)
        id: repo
        run: |
          echo "owner=$(echo '${{ github.repository_owner }}' | tr '[:upper:]' '[:lower:]')" >> $GITHUB_OUTPUT

      - name: Set output variables
        id: vars
        run: echo "short_sha=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ steps.repo.outputs.owner }}/cruder
          tags: |
            type=sha,format=short
            type=raw,value=latest

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64 # build for amd64 architecture as GKE nodes run on amd64
          push: true
          tags: |
            ${{ env.REGISTRY }}/${{ steps.repo.outputs.owner }}/cruder:${{ steps.vars.outputs.short_sha }}
            ${{ env.REGISTRY }}/${{ steps.repo.outputs.owner }}/cruder:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Image digest
        run: |
          echo "âœ… Built and pushed image:"
          echo "${{ env.REGISTRY }}/${{ steps.repo.outputs.owner }}/cruder:${{ steps.vars.outputs.short_sha }}"

  # Job 2: Deploy to Staging (Portfolio Showcase Mode)
  # This job runs successfully to demonstrate deployment workflow structure
  # without requiring actual GKE infrastructure
  deploy-staging:
    name: Deploy to Staging (Showcase)
    runs-on: ubuntu-latest
    needs: build
    environment:
      name: staging
      url: https://github.com/${{ github.repository }}

    steps:
      - name: Portfolio Showcase Mode
        run: |
          echo "ğŸ“‹ PORTFOLIO SHOWCASE MODE"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          echo "This workflow demonstrates production-ready deployment practices:"
          echo ""
          echo "âœ… Progressive deployment (staging â†’ production)"
          echo "âœ… Environment isolation with approval gates"
          echo "âœ… Zero-downtime rolling updates"
          echo "âœ… Automated health checks and smoke tests"
          echo "âœ… GKE Autopilot deployment strategy"
          echo ""
          echo "To enable actual deployments:"
          echo "  1. Add GKE_SA_KEY secret to repository settings"
          echo "  2. Update GKE_CLUSTER and GKE_REGION variables"
          echo "  3. Replace this job with the commented deployment code below"
          echo ""
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "âœ… Staging deployment workflow validated"

  # # ACTUAL DEPLOYMENT CODE (commented for portfolio showcase)
  # # Uncomment and replace the job above to enable real deployments
  # deploy-staging:
  #   name: Deploy to Staging
  #   runs-on: ubuntu-latest
  #   needs: build
  #   environment:
  #     name: staging
  #     url: http://<STAGING_IP>  # Replace with your staging Load Balancer IP

  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v4

  #     - name: Authenticate to Google Cloud
  #       uses: google-github-actions/auth@v2
  #       with:
  #         credentials_json: ${{ secrets.GKE_SA_KEY }}

  #     - name: Set up Cloud SDK
  #       uses: google-github-actions/setup-gcloud@v2

  #     - name: Install gke-gcloud-auth-plugin
  #       run: |
  #         gcloud components install gke-gcloud-auth-plugin

  #     - name: Get GKE credentials
  #       run: |
  #         gcloud container clusters get-credentials ${{ env.GKE_CLUSTER }} \
  #           --region=${{ env.GKE_REGION }}

  #     - name: Update deployment image
  #       run: |
  #         kubectl set image deployment/cruder-app \
  #           cruder-app=${{ env.REGISTRY }}/${{ needs.build.outputs.owner }}/cruder:${{ needs.build.outputs.short-sha }} \
  #           -n staging

  #     - name: Wait for rollout to complete
  #       run: |
  #         kubectl rollout status deployment/cruder-app -n staging --timeout=300s

  #     - name: Verify deployment
  #       run: |
  #         echo "Checking deployment status..."

  #         # Get pod status (for visibility, but don't fail on transient states)
  #         kubectl get pods -n staging -l app=cruder

  #         # Verify all replicas are ready using deployment-level status (not pod-level)
  #         # Why: During rolling updates, kubectl wait with pod labels matches both old (terminating)
  #         # and new pods, causing "pod not found" errors. Deployment status only counts pods
  #         # from the current ReplicaSet, avoiding race conditions. See docs/incidents/2025-01-27-deployment-verification-fix.md
  #         READY_REPLICAS=$(kubectl get deployment cruder-app -n staging -o jsonpath='{.status.readyReplicas}')
  #         DESIRED_REPLICAS=$(kubectl get deployment cruder-app -n staging -o jsonpath='{.spec.replicas}')

  #         echo "Ready replicas: $READY_REPLICAS/$DESIRED_REPLICAS"

  #         if [ "$READY_REPLICAS" -eq "$DESIRED_REPLICAS" ]; then
  #           echo "âœ… All replicas are ready"
  #         else
  #           echo "âŒ Not all replicas are ready"
  #           exit 1
  #         fi

  #     - name: Run smoke tests
  #       run: |
  #         echo "Running smoke tests against staging..."

  #         # Test health endpoint
  #         kubectl run test-pod --image=curlimages/curl:latest --rm -i --restart=Never -- \
  #           curl -f http://cruder-service.staging.svc.cluster.local/health || exit 1

  #         # Test ready endpoint
  #         kubectl run test-pod --image=curlimages/curl:latest --rm -i --restart=Never -- \
  #           curl -f http://cruder-service.staging.svc.cluster.local/ready || exit 1

  #         echo "âœ… Smoke tests passed"

  #     - name: Deployment summary
  #       run: |
  #         echo "::notice::âœ… Successfully deployed to staging"
  #         echo "::notice::Image: ${{ env.REGISTRY }}/${{ needs.build.outputs.owner }}/cruder:${{ needs.build.outputs.short-sha }}"
  #         echo "::notice::URL: http://<STAGING_IP>"  # Replace with your staging Load Balancer IP

  # Job 3: Deploy to Production (Portfolio Showcase Mode)
  # This job requires manual approval and demonstrates production safeguards
  deploy-production:
    name: Deploy to Production (Showcase)
    runs-on: ubuntu-latest
    needs: [build, deploy-staging]
    environment:
      name: production
      url: https://github.com/${{ github.repository }}

    steps:
      - name: Portfolio Showcase Mode
        run: |
          echo "ğŸš€ PRODUCTION DEPLOYMENT - PORTFOLIO SHOWCASE"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          echo "This job demonstrates production-grade safeguards:"
          echo ""
          echo "âœ… Manual approval gate (GitHub Environment protection)"
          echo "âœ… Dependency on successful staging deployment"
          echo "âœ… Zero-downtime rolling update strategy"
          echo "âœ… Automated smoke tests before serving traffic"
          echo "âœ… Instant rollback capability on failure"
          echo ""
          echo "Production deployment would include:"
          echo "  â€¢ kubectl set image with new container"
          echo "  â€¢ Health probe verification (liveness + readiness)"
          echo "  â€¢ Automated smoke tests (health + ready endpoints)"
          echo "  â€¢ Rollout status monitoring with 600s timeout"
          echo ""
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "âœ… Production deployment workflow validated"

  # # ACTUAL DEPLOYMENT CODE (commented for portfolio showcase)
  # # Uncomment and replace the job above to enable real deployments
  # deploy-production:
  #   name: Deploy to Production
  #   runs-on: ubuntu-latest
  #   needs: [build, deploy-staging]
  #   environment:
  #     name: production
  #     url: http://<PRODUCTION_IP>  # Replace with your production Load Balancer IP

  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v4

  #     - name: Authenticate to Google Cloud
  #       uses: google-github-actions/auth@v2
  #       with:
  #         credentials_json: ${{ secrets.GKE_SA_KEY }}

  #     - name: Set up Cloud SDK
  #       uses: google-github-actions/setup-gcloud@v2

  #     - name: Install gke-gcloud-auth-plugin
  #       run: |
  #         gcloud components install gke-gcloud-auth-plugin

  #     - name: Get GKE credentials
  #       run: |
  #         gcloud container clusters get-credentials ${{ env.GKE_CLUSTER }} \
  #           --region=${{ env.GKE_REGION }}

  #     - name: Update deployment image
  #       run: |
  #         kubectl set image deployment/cruder-app \
  #           cruder-app=${{ env.REGISTRY }}/${{ needs.build.outputs.owner }}/cruder:${{ needs.build.outputs.short-sha }} \
  #           -n production

  #     - name: Wait for rollout to complete
  #       run: |
  #         kubectl rollout status deployment/cruder-app -n production --timeout=600s

  #     - name: Verify deployment
  #       run: |
  #         echo "Checking deployment status..."

  #         # Get pod status (for visibility, but don't fail on transient states)
  #         kubectl get pods -n production -l app=cruder

  #         # Verify all replicas are ready using deployment-level status (not pod-level)
  #         # Why: During rolling updates, kubectl wait with pod labels matches both old (terminating)
  #         # and new pods, causing "pod not found" errors. Deployment status only counts pods
  #         # from the current ReplicaSet, avoiding race conditions. See docs/incidents/2025-01-27-deployment-verification-fix.md
  #         READY_REPLICAS=$(kubectl get deployment cruder-app -n production -o jsonpath='{.status.readyReplicas}')
  #         DESIRED_REPLICAS=$(kubectl get deployment cruder-app -n production -o jsonpath='{.spec.replicas}')

  #         echo "Ready replicas: $READY_REPLICAS/$DESIRED_REPLICAS"

  #         if [ "$READY_REPLICAS" -eq "$DESIRED_REPLICAS" ]; then
  #           echo "âœ… All replicas are ready"
  #         else
  #           echo "âŒ Not all replicas are ready"
  #           exit 1
  #         fi

  #     - name: Run smoke tests
  #       run: |
  #         echo "Running smoke tests against production..."

  #         # Test health endpoint
  #         kubectl run test-pod --image=curlimages/curl:latest --rm -i --restart=Never -- \
  #           curl -f http://cruder-service.production.svc.cluster.local/health || exit 1

  #         # Test ready endpoint
  #         kubectl run test-pod --image=curlimages/curl:latest --rm -i --restart=Never -- \
  #           curl -f http://cruder-service.production.svc.cluster.local/ready || exit 1

  #         echo "âœ… Smoke tests passed"

  #     - name: Deployment summary
  #       run: |
  #         echo "::notice::ğŸš€ Successfully deployed to PRODUCTION"
  #         echo "::notice::Image: ${{ env.REGISTRY }}/${{ needs.build.outputs.owner }}/cruder:${{ needs.build.outputs.short-sha }}"
  #         echo "::notice::URL: http://<PRODUCTION_IP>"  # Replace with your production Load Balancer IP

  #     - name: Notify on failure
  #       if: failure()
  #       run: |
  #         echo "::error::âŒ Production deployment failed!"
  #         echo "::error::Consider rolling back with: kubectl rollout undo deployment/cruder-app -n production"
 